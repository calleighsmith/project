---
title: "Analyzing the Environmental Factors of U.S. Beaches"
author: "Calleigh Smith"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=F, warning=F)
library(MASS)
library(tidyverse)
library(splitstackshape)
library(janitor)
library(gganimate)
library(gifski)
library(viridis)
library(patchwork)
library(magick)
library(AER)
library(DHARMa)
library(vcd)
library(pscl)
library(lme4)
library(shiny)
library(gtsummary)
library(kableExtra)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(maps)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(ggiraph)
```

```{r, message = F, error=F, include=F}
action_duration <- read_csv("data/action_duration.csv") %>%
  clean_names() %>%
  dplyr::select(1:7)
advisory_and_monitoring <- read_csv("data/advisory_and_monitoring.csv") %>%
  clean_names()
beach_actions <- read_csv("data/beach_actions_(advisories_and_closures).csv") %>%
  clean_names()
beach_attributes <- read_csv("data/beach_attributes.csv") %>%
  clean_names()
beach_days <- read_csv("data/beach_days.csv") %>%
  clean_names()
beach_monitoring_frequency <- read_csv("data/beach_monitoring_frequency.csv") %>%
  clean_names()
beach_profile <- read_csv("data/beach_profile_list.csv") %>%
  clean_names()
beach_wqs_criteria_names_and_values <- read_csv("data/beach_wqs_criteria_names_and_values.csv") %>%
  clean_names()
local_action_decision_procedures <- read_csv("data/local_action_decision_procedures.csv") %>%
  clean_names()
possible_pollution_sources <- read_csv("data/possible_pollution_sources.csv") %>%
  clean_names()
tier_1_stats <- read_csv("data/tier_1_stats.csv") %>%
  clean_names()
water_quality_report <- read_csv("data/water_quality_report.csv") %>%
  clean_names()
```

```{r,include=F}
tier_1_beach_ids <- tier_1_stats$beach_id
beaches <- beach_profile %>%
  filter(beach_id %in% tier_1_beach_ids,
         year == 2020)

beaches <- stratified(beaches, 7,.5) %>%
  pull(beach_id)

test_set <- setdiff(tier_1_beach_ids, beaches)
```

```{r data-wrangling-q1}
ba <- beach_attributes %>%
  filter(beach_id %in% beaches) %>%
  dplyr::select(year, beach_id, beach_length_mi) %>%
  mutate(beach_length_mi = ifelse(beach_length_mi == "-", NA_character_, beach_length_mi)) %>%
  fill(beach_length_mi) %>%
  mutate(beach_length_mi = as.numeric(beach_length_mi))

bp <- beach_profile %>%
  filter(beach_id %in% beaches) %>%
  dplyr::select(year, beach_id, waterbody_name, waterbody_type, swim_status, beach_access, beach_owner) %>%
  mutate(beach_access = ifelse(beach_access == "null", NA_character_, beach_access),
         beach_owner = ifelse(beach_owner == "-", NA_character_, beach_owner),
         waterbody_name = ifelse(waterbody_name == "-", NA_character_, waterbody_name),
         waterbody_type = ifelse(waterbody_type == "-", NA_character_, waterbody_type))

watertype <-  beach_wqs_criteria_names_and_values %>%
  filter(beach_id %in% beaches) %>%
  dplyr::select(year, beach_id, water_type)

t1stats <- tier_1_stats %>%
  filter(beach_id %in% beaches) %>%
  dplyr::select(year, beach_id, no_of_days_under_beach_action, swim_season_beach_days)

risk1 <- inner_join(ba, bp)
risk2 <- inner_join(risk1, watertype)
risk <- inner_join(risk2, t1stats) %>%
  filter(no_of_days_under_beach_action < 365) %>%
  group_by(beach_id) %>%
  slice(which.max(year))
```

# Introduction
## Overview

According to he National Ocean Service, "Our oceans and coasts touch every American every day - providing us with places to live, food to eat, jobs, commerce, recreation, energy, even medicines that heal" [1]. Unfortunately, waters globally are highly vulnerable to human actions and exposed to threats such as pollution, climate change, algae blooms, coastal development, and more. In order to ensure the health of these natural environments and those who depend on them, it is essential that we understand the current status of coastal waters and which beaches are at risk for the future.

This paper will identify risk factors for the environmental degradation of beaches, which can in turn guide legislators to develop regulations regarding these bodies of water. Additionally, environmental protection of the oceans and coasts is futile without the education and engagement of the public. Public knowledge of coastal conservation is limited. According to a survey by Lotze et al., 70% of respondents believe the marine environment is under threat from human activities, but only 15% thought the ocean's health was poor or threatened [2]. By statistically analyzing data relating to the oceans and the coasts and presenting it in a way that can be understood by a general audience, this project has the ability to illustrate a clear picture of the threats to aquatic bodies in the U.S. and inform marine managers, policy makers, conservation practitioners, and educators to improve marine management and conservation programs.

In our analysis, we will look at the following research questions:

- **Measuring Risk Factors:** Can we predict the number of beach actions (beach-specific advisories or closings issued by the reporting state or local governments) based on a beach's characteristics? Similarly, are certain variables more valuable in predicting beach actions?
- **Measuring Pollution**: How are the different types of pollution (algal, animal, sewer line, etc.) distributed across U.S. waters? Are certain regions more prone to certain pollution types of pollution than others?
- **Measuring Government Communication with Locals**: How do local governments communicate with citizens when there is a beach advisory or closing? Are governments transparent in reporting the risks associated with local bodies of water to residents?

## Data

Our data were obtained through the Environmental Protection Agency (EPA) as part of their BEACON 2.0 online system [3]. The EPA created the BEach Advisory and Closing Online Notification (BEACON) to meet the Agency's requirement to provide the public a database of pollution occurrences for coastal recreational waters. Under the Beaches Environmental Assessment and Coastal Health (BEACH) Act of 2000, EPA provides annual grants to coastal and Great Lakes states, territories, and eligible tribes to help local authorities monitor their coastal and Great Lakes beaches and notify the public of water quality conditions that may be unsafe for swimming. BEACON contains state-reported beach monitoring and notification data and is fully available online without the need for special permissions.

The BEACON 2.0 online tool provides many reports on all beaches in the United States. Available datasets include those related to each action for a beach, beach attributes and profiles, beach days, beach monitoring frequency, possible pollution sources, water quality, tier 1 beach information state summaries, and more. Using BEACON 2.0's user guide found [here](https://www.epa.gov/sites/production/files/2014-08/documents/beacon_usersguide.pdf), one can view a full data dictionary, information on how to use the database, and basic summary statistics on some of the variables.

In this analysis, we will mostly focus on Tier 1 beaches, which tend to have the highest risk and the most available data across all of the datasets. States and territories designate their significant public beaches as Tier 1 beaches, which is a requirement of the BEACH Act grant program. In order to deal with issues of spatial correlation, we will take a stratified random sample of 50% of the beaches from each water body where data were available from the year 2020, which results in about 1,070 beaches in this analysis.

For our first research question on measuring risk factors, we will look at the number of days under action in a given year for each beach as the response variable. Features we would like to consider include action reason, beach ownership (private/public), beach length, water body type, beach monitoring frequency, water quality, and number of swim days. For our second research question, we will look at states, names of body waters, possible pollution sources, and water quality standards. For our third research question, we will look at local action decision procedures, which includes how governing bodies communicated beach actions with residents, and the reason/severity of the beach action. 

Additionally, we will use data from a supplemental source, "A Criteria-Based Evaluation of Environmental Literacy Plans in the United States," a dissertation by Karena Ruddiero at the University of Tennessee - Knoxville (2016) [4], which includes a dataset of all U.S. states and a score of the environmental literacy for that state based on several factors, including the integration of environmental literacy in current state curricula, graduation requirements for environmental literacy, and political status. Combining local action decision procedures for beach actions and a state's environmental literacy score will provide a more complete picture of government transparency when it comes to marine health and the public.

## Descriptive Analysis

The first two research questions revolve around predicting the number of days under action for a beach and how pollutants are distributed, respectively. In the first map below, one can see that the number of days under action vary depending on the year and region. In particular, it appears that the Gulf of Mexico and the West coast have a particularly high concentration of beaches with many days under action.  In the second map below, one can see that the number of pollutants vary over time and by region.  The most obvious observation is that from 2016 onward, it appears that the beaches of Southern California have experienced more pollution sources than in previous years. Both of these maps provide some initial insight into the state of beaches historically to present times and suggest that there may be a relationship between number of days under action and pollutants depending on the time period and region.

```{r}
us = map_data("state")

action_duration <-action_duration %>%
  filter(beach_id %in% beaches,
         no_of_days_under_action <= 365,
         no_of_days_under_action >= 0
         ) %>%
  left_join(beach_attributes, by = c("beach_id", "year")) %>%
  filter(start_latitude > 23,
         start_longitude < which.min(us$long))


beach_actions_t1 <-beach_actions %>%
  filter(beach_id %in% beaches) %>%
  left_join(beach_attributes, by = c("beach_id", "year")) %>%
  filter(start_latitude > 23,
         start_longitude < which.min(us$long)) %>%
  mutate(no_pollutants = case_when(
    action_reasons == "-" ~ NA_integer_,
    TRUE ~ lengths(strsplit(action_reasons, ", "))
  )) 
```

```{r}
a <- ggplot() +
  geom_map(data=us, map=us, aes(x=long, y=lat, map_id=region), 
           color="grey", fill=NA) +
  geom_point(data=action_duration, aes(x=as.double(start_longitude), 
                                       y=as.double(start_latitude),
                                       color=as.integer(no_of_days_under_action), 
                                       size=as.integer(no_of_days_under_action)), 
             alpha = 0.5) +
  labs(color = "Number of Days Under Action", 
       title = "West Coast and Gulf of Mexico Have Most Number of Days Under Action", 
       subtitle = "Year: {frame_time}") +
  transition_time(year) +
  scale_color_viridis(option="viridis", direction = -1, guide = guide_legend(), begin=.1) +
  ease_aes('linear')+
  scale_size(guide = FALSE) + # no legend for size
  theme_void()+
  theme(legend.position="bottom")

a_gif = animate(a, duration = 21, nframes=21)

b<- ggplot() +
  geom_map(data=us, map=us, aes(x=long, y=lat, map_id=region), 
           color="grey", fill=NA) +
  geom_point(data=beach_actions_t1, aes(x=as.double(start_longitude), 
                                     y=as.double(start_latitude),
                                     color=as.integer(no_pollutants), 
                                     size=as.integer(no_pollutants)+2), 
             alpha = .1) +
  labs(color = "Number of Pollutants", 
       title = "Number of Pollutants Increases in California Since 2016", 
       subtitle = "Year: {frame_time}") +
  transition_time(year) +
  scale_color_viridis(option="viridis", direction = -1, guide = guide_legend(), begin=.1) +
  ease_aes('linear')+
  scale_size(guide = FALSE) + 
  theme_void()+
  theme(legend.position="bottom")

b_gif = animate(b, duration=21, nframes=21)
  
a_mgif <- image_read(a_gif)
b_mgif <- image_read(b_gif)

new_gif <- image_append(c(a_mgif[1], b_mgif[1]))
for(i in 2:21){
  combined <- image_append(c(a_mgif[i], b_mgif[i]))
  new_gif <- c(new_gif, combined)
}

new_gif
```
*Note: The size of the points in this visualization correspond to the number of beach actions or number of pollutants (the same as the color representation). This was done for ease of identification of the more extreme values.

In the following plot, one can observe the ways that local governments communicate with residents about actions which affect community beaches. This plot can provide some preliminary insight into our last research question, which revolves around the transparency of local governments in reporting news about beaches to citizens. From this plot, it appears that the most common methods of communication by local governments include posting notifications on the internet or at the beaches themselves.

```{r}
p1<- ggplot(risk, aes(x=no_of_days_under_beach_action))+
  geom_histogram(fill ="seagreen1", color="black") +
  labs(title = "Number of Days Under Beach Action\nIs Right-Skewed",
        y = "Number of Beach-Year Combinations", 
        x = "Number of Days") +
  theme_minimal()

#p1
```

```{r, fig.width=8.5, fig.height=4,fig.align='center'}
local_action_decision_procedures[local_action_decision_procedures=="-"] <- "0"
local_action_decision_procedures[local_action_decision_procedures=="Yes"] <- "1"
local_action_decision_procedures <- local_action_decision_procedures %>%
  mutate(across(9:16, as.double)) %>%
  group_by(beach_id) %>% 
  filter(year == max(year))

methods = c("Posted on Internet", "Posted at Beach", "Sent to Phone List", "Announced on TV", "Announced on Radio", "Published in Newspaper", "Results on Request", "Other")
methods_yes = c(3137,2644,1355,1204,1168,1081,236,55)
eda_methods <- cbind(methods, methods_yes) %>%
  as_tibble() %>%
  mutate(methods_yes = as.numeric(methods_yes),
         methods = fct_relevel(methods, levels = c("Other", "Results on Request", "Published in Newspaper", "Announced on Radio", "Announced on TV", "Sent to Phone List", "Posted at Beach", "Posted on Internet")))

ggplot(eda_methods, aes(x = methods, y = methods_yes)) +
  geom_bar(stat = "identity", fill = "seagreen1") +
  geom_text(aes(label=methods_yes), hjust=0)+
  coord_flip()+
  theme_minimal() +
  labs(y = "Number of Beaches", x = "Method of Communication", title = "Most Common Method of Communicating Beach Actions to Citizens\nIs via Posting on the Internet")
```

# Methodology

## Measuring Risk Factors

For this research question, the main goal is to predict the number of days under beach action for Tier 1 beaches in our random sample.  We will mainly consider features related to the physical characteristics of the beach. These features include the beach length, whether the beach is privately owned, whether the beach is accessible to the public, the water body and type of water body that the beach belongs to, the swim status of the beach, swim season length, the water type of the beach (i.e. marine or freshwater), and the number of days in the swim season. We will use the most recent observation from our random sample of Tier 1 Beaches ($n$ = 391).

Upon inspection, it appears that there are several outliers in the data, specifically, beaches where there are many days in a year under beach action. We did not think it would be wise to remove these outliers, as it is meaningful to be able to predict the number of days under beach action for these extreme cases.

Our response variable, number of days under beach action, is a discrete count which is also highly skewed right. Since over-dispersion appeared to be an issue, a Poisson model was deemed to not be ideal. However, a Negative Binomial (NB) model, which is often used instead of a Poisson model to deal with the issue of over-dispersion, appeared to be a worthy alternative.  Additionally, our outcome is zero-inflated, as confirmed by a zero inflation test on the residuals, and there is evidence of over-dispersion. To test for this, we performed an ANOVA test using AIC as the criterion to decipher whether a zero-inflated NB model or a regular NB model performed better, and the zero-inflated NB model had a lower AIC.  Therefore, in order to model the number of days under beach action, we will use a zero-inflated Negative Binomial model. 

Our outcome, number of days under beach action, follows a NB distribution. We confirmed this by creating an ord plot, which helps in identifying which count data model is underlying, and obtained a positive slope and intercept indicative of a negative binomial distribution. Upon inspection of the residual plots, specifically the QQ plot of the deviance residuals and the plot of the residuals vs. fitted values, the model assumptions appear satisfactory.

In fitting our zero-inflated Negative Binomial model, we ran into the issue of collinearity among the predictors. Using VIF, we were able to identify that the variables water type (i.e. freshwater, marine, both), water body type (i.e. Sound/Bay/Inlet, Still Water, Open Coast), and water body name were highly correlated. Therefore, we chose water body name, as this is the most specific of the three variables.

One consideration in approaching this research question was whether it was appropriate to include an offset term based on the number of days a beach is "eligible" for beach actions. However, a major class of beach actions include rain and weather advisories, like hurricanes, that can affect beaches during their non-swimming seasons. For example, Atlantic hurricane season takes place from June through November, whereas swimming season on Long Island, NY, which is on the Atlantic ocean, takes place from June through September (Memorial Day to Labor Day). Therefore, we decided that it would be too restrictive to include an offset term.

The full model specification is as follows:
$$
\begin{aligned}
\mu_i = \text{exp}(\text{ln}(t_i) &+ \beta_0 + \beta_1\times \text{beach length}_i +\beta_2\times I(\text{water body name}_i = \text{Chesapeake Bay}) \\
&+ \beta_3\times I(\text{water body name}_i = \text{Gulf of Mexico}) + \beta_4\times I(\text{water body name}_i = \text{Great Lakes}) \\
&+ \beta_{5}\times I(\text{water body name}_i = \text{Long Island Sound}) + \beta_{6}\times I(\text{water body name}_i = \text{Pacific Ocean}) \\ &+  \beta_{7}\times I(\text{swim status}_i = \text{No Advisory or Closure})+
\beta_{8}\times I(\text{beach owner}_i = \text{Public}) + \beta_{9}\times \text{days in swim season}_i).
\end{aligned}
$$
Using this notation, the fundamental negative binomial regression model for a beach $i$ is written as $$Pr(Y=y_i\mid\mu_i, \alpha) =\frac{\Gamma(y_i +\alpha^{-1})}{\Gamma(\alpha^{-1})\Gamma(y_i+1)}\left(\frac{1}{1+\alpha\mu_i}\right)^{\alpha^{-1}}\left(\frac{\alpha\mu_i}{1+\alpha\mu_i}\right)^{y_i}.$$

Finally, as prediction is a secondary goal of this analysis, we will create a test set comprised of the second most recent observation for each tier 1 beach as our test set. We will then use our model to predict the number of days under beach action for the test set, which will give us some insight into our model's predictive accuracy using metrics such as the mean square error.

## Measuring Pollution

In this question, we will seek to better understand how pollution is distributed across US waters, such as in the Pacific Ocean, Atlantic Ocean, Gulf of Mexico, etc. Our response variable is the number of pollutants found at a given beach since 2015 (note that pollutants can repeat if they return the next year). Our main variable of interest will be water body name, but we will include other covariates from our first research question as controls in our model as well as the action type as a result of the pollution.

One issue with model fitting of this type is that the observations are not independent because pollutants at beaches within the same main body of water are likely correlated. Therefore, the independence assumption that most simple generalized linear models require to fit is violated.  One might consider an aggregated approach, where rather than considering the individual beaches in a body of water, we consider the average of all beaches within a body of water. While this aggregated approach would be independent, it would not take advantage of all of the data. In our dataset, there are 12 water bodies, so this aggregate approach would only consider 12 data points, which would result in very low power. Another approach would be to fit 12 separate models, one for the beaches in each body of water. However, this results in multiple regression models for each body of water and does not take advantage of the information in the data from other bodies of water. This can also add noise to the estimates in each model, as the models are not built off of that much data.

One solution is to create a random intercepts linear mixed model (LMM). In our analysis, it is reasonable enough to assume that beaches in different bodies of water are not correlated (although this is not entirely true and will be discussed as a limitation later in the Discussion). LMMs depend on fixed and random effects. In our model, the fixed effects are the normal explanatory variables like in simple linear regression, and we will consider the fixed effects as covariates. The random effect is the variable that seeks to explain our grouping factor, which in our case is the body of water. The model assumptions for LMMs are similar to simple linear regression. There should be no patterns in the residuals vs. fitted plot and normality in the residuals. 

The full model specification is as follows for the $i$-th beach for the $j$-th body of water:

$$
\begin{aligned}
Y_{ij} = (\gamma_{00}+u_{0j})&+\gamma_{10}\text{Beach Length}_{ij} + \gamma_{20}I(\text{Beach Access}_{ij} = \text{Public}) + \gamma_{30}I(\text{Beach Owner}_{ij} = \text{Public}) \\
&+ \gamma_{40}\text{Swim Season Beach Days}_{ij} + \gamma_{50}I(\text{Action Type}_{ij} = \text{Contamination Advisory})\\
&+\gamma_{60}I(\text{Action Type}_{ij} = \text{Permanent Closure}) + \gamma_{70}I(\text{Action Type}_{ij} = \text{Posing}) \\
&= \gamma_{80}I(\text{Action Type}_{ij} = \text{Rain Advisory})
\end{aligned}
$$
where $u_{0j}$ is the random effect term. Using this modeling approach, we will be able to decipher how much of the variance in total pollutants at beaches can be explained by the body of water that these beaches find themselves situated near. We will present the variation among the bodies of water visually by plotting the overall model estimate for the intercepts.

## Measuring Government Communication with Locals

For this question, we will use a descriptive analysis approach. Our goal is to look at how governments communicate with their citizens about their beaches. To approach this question, for each state, we will find the proportion of beaches in that state which use each of the following modes of communication: posting on the internet, posting at the beach, sending to a phone list, announcing in the radio, announcing on TV, announcing on radio, providing results upon request, and other. We will only consider the most recent observation for each beach, as this probably include the most current methods of communication used by that beach. We also have information regarding the environmental literacy rating for each state.

Using this information, we will create a Shiny app. In our Shiny app, the user will be able to select which method of communication that they are interested in. Then, a plot of the Mainland USA will appear with the states filled in by proportion of beaches in that state which use that method of communication. Additionally, the user will see a point in the center of each state whose size represents the environmental literacy rating in that state.

```{r}
state_long_lat <- read_csv("data/state_long_lat.csv")
environmental_literacy <- read_csv("data/environmental_literacy.csv") %>%
  mutate(level = case_when(
    rating >= 0.5 ~ "Good",
    rating < 0.5 ~ "Poor"
  ),
  state = ifelse(state == "DC", "District of Columbia", state))

environmental_literacy <- left_join(environmental_literacy, state_long_lat, by = c("state" = "name")) %>%
  filter(!state %in% c("Hawaii", "Alaska"))

states_comms <- local_action_decision_procedures %>%
  filter(!state_code %in% c("AS", "GU", "MP", "VI")) %>%
  mutate(state_code = case_when(
    state_code %in% c("MK", "ST")~ "WA",
    state_code == "BR" ~ "WI",
    state_code == "GP" ~ "MN",
    TRUE ~ as.character(state_code))) %>%
  group_by(state_code) %>%
  summarize(prob_internet = mean(posted_on_internet), prob_newspaper = mean(published_in_newspaper),
         prob_phone = mean(sent_to_phone_l_ist), prob_radio = mean(announcedon_radio),
         prob_beach = mean(posted_atthe_beach), prob_tv = mean(announcedon_tv), 
         prob_request = mean(provided_resultson_request), prob_other = mean(other))
```

# Results

## Measuring Risk

Below one can find the results from the zero-inflated negative binomial model which predicts the number of days under beach action using features related to beach attributes.

```{r, fig.align='center'}
#m_pois <- glm(no_of_days_under_beach_action ~ . -year -beach_id, risk, family = "poisson")
#summary(m_pois)
#dispersiontest(m_pois) #over-dispersion is present

risk <- risk %>%
  filter(waterbody_name != "Inland")%>%
  mutate(waterbody_name = case_when(
    waterbody_name == "Chesapeake Bay" ~ "Atlantic Ocean",
    waterbody_name %in% c("Lake Erie", "Lake Huron", "Lake Michigan", "Lake Ontario", "Lake Superior") ~ "Great Lakes",
    TRUE ~ as.character(waterbody_name)
  )) %>%
  ungroup() %>%
  select(-beach_id)
  
labels <- c("Intercept", "Beach Length (mi)", "Water Body (Chesapeake Bay)", "Great Lakes", "Water Body (Gulf of Mexico)",
                  "Water Body (Long Island Sound)", "Water Body (Pacific Ocean)", "Swim Status (No advisory or closure)",  "Beach Owner (Public)", "Swim Season (day)", "Intercept", "Beach Length (mi)", "Water Body (Chesapeake Bay)", "Great Lakes", "Water Body (Gulf of Mexico)",
                  "Water Body (Long Island Sound)", "Water Body (Pacific Ocean)", "Swim Status (No advisory or closure)",  "Beach Owner (Public)", "Swim Season (day)")

mod_names = c("count_(Intercept)", "count_beach_length_mi", "count_waterbody_nameChesapeakeBay", "count_waterbody_nameGreat Lakes", "count_waterbody_nameGulf of Mexico", "count_waterbody_nameLong Island Sound", "count_waterbody_namePacific Ocean", "count_swim_statusNo advisory or closure", "count_beach_ownerPublic", "count_swim_season_beach_days",
              "zero_(Intercept)", "zero_beach_length_mi", "zero_waterbody_nameChesapeakeBay", "zero_waterbody_nameGreat Lakes", "zero_waterbody_nameGulf of Mexico", "zero_waterbody_nameLong Island Sound", "zero_waterbody_namePacific Ocean", "zero_swim_statusNo advisory or closure", "zero_beach_ownerPublic", "zero_swim_season_beach_days")

m<- glm.nb(no_of_days_under_beach_action ~ . -year -water_type-waterbody_type-beach_access-swim_status, risk)

m2 <- zeroinfl(no_of_days_under_beach_action ~ . -year  -water_type-waterbody_type-beach_access-swim_status, risk, dist="negbin")
tab_model(m2, digits = 4, auto.label = F,
          dv.labels = "Number of Days Under Beach Action"
)
```

As prediction is also a motive for this research question, below we investigate this model's ability to predict the number of days under beach action for both the training and test data.

```{r data-wrangling-test, fig.width=8, fig.height=4}
ba_t <- beach_attributes %>%
  filter(beach_id %in% test_set) %>%
  dplyr::select(year, beach_id, beach_length_mi) %>%
  mutate(beach_length_mi = ifelse(beach_length_mi == "-", NA_character_, beach_length_mi)) %>%
  fill(beach_length_mi) %>%
  mutate(beach_length_mi = as.numeric(beach_length_mi))

bp_t <- beach_profile %>%
  filter(beach_id %in% test_set) %>%
  dplyr::select(year, beach_id, waterbody_name, waterbody_type, swim_status, beach_access, beach_owner) %>%
  mutate(beach_access = ifelse(beach_access == "null", NA_character_, beach_access),
         beach_owner = ifelse(beach_owner == "-", NA_character_, beach_owner),
         waterbody_name = ifelse(waterbody_name == "-", NA_character_, waterbody_name),
         waterbody_type = ifelse(waterbody_type == "-", NA_character_, waterbody_type))

watertype_t <-  beach_wqs_criteria_names_and_values %>%
  filter(beach_id %in% test_set) %>%
  dplyr::select(year, beach_id, water_type)

t1stats_t <- tier_1_stats %>%
  filter(beach_id %in% test_set) %>%
  dplyr::select(year, beach_id, no_of_days_under_beach_action, swim_season_beach_days)

risk1_t <- inner_join(ba_t, bp_t)
risk2_t <- inner_join(risk1_t, watertype_t)
risk_t <- inner_join(risk2_t, t1stats_t) %>%
  filter(no_of_days_under_beach_action < 365) %>%
  group_by(beach_id) %>%
  slice(which.max(year)) %>%
  ungroup() %>%
  filter(waterbody_name != "Inland")%>%
  mutate(waterbody_name = case_when(
    waterbody_name %in% c("Lake Erie", "Lake Huron", "Lake Michigan", "Lake Ontario", "Lake Superior") ~ "Great Lakes",
    waterbody_name == "Chesapeake Bay" ~ "Atlantic Ocean",
    TRUE ~ as.character(waterbody_name)
  ))

train_preds <- predict(m2, type="response", na.action=na.pass)
train_true <- risk$no_of_days_under_beach_action
pred_lab_train <- rep("Predicted",length(train_preds))
true_lab_train<- rep("True", length(train_true))
lab_train <- c(pred_lab_train, true_lab_train)
vals_train = c(train_preds, train_true)
train_rmse <- mean((train_true - train_preds) ^ 2)

test_preds <- predict(m2, risk_t, type = "response", na.action = na.pass)
test_true <- risk_t$no_of_days_under_beach_action
pred_lab_test <- rep("Predicted", length(test_preds))
true_lab_test<- rep("True", length(test_true))
lab_test <- c(pred_lab_test, true_lab_test)
test_rmse <- mean((test_true - test_preds) ^ 2)
vals_test <- c(test_preds, test_true)

train_vals <- data_frame(vals_train, lab_train)
test_vals <- data_frame(vals_test, lab_test)

plot1 <- ggplot() + 
  geom_histogram(data=train_vals, aes(x=vals_train,  fill=lab_train), alpha=0.5) + 
  theme_minimal()+
  labs(title="Training Set", x = "No. Days Under Beach Action",
       y="Number of Beaches", fill="Value")

plot2 <- ggplot() + 
  geom_histogram(data=test_vals, aes(x=vals_test,  fill=lab_test), alpha=0.5) + 
  theme_minimal()+
  labs(title="Testing Set", x = "No. Days Under Beach Action",
       y="Number of Beaches", fill="Value")

(plot1 + plot2)  + plot_annotation(tag_levels = 'A', title = "Evaluating Zero-Inf. NB Model's Predictive Accuracy")+ 
  plot_layout(guides = 'collect')

```

While inference is the main goal of this model, it is informative to have some information about how well this model predicts how many days a beach will be under action. Using this model, the test RMSE is `r round(test_rmse, 3)`, which is more than the train RMSE, which is `r round(train_rmse,3)`. As a result, it appears like our model may be overfitting. We will explore the discrepancies between the fitted and true values in the Discussion.

## Measuring Pollution

Unfortunately, the results for this section are inconclusive. After we aggregated the number of pollutants for the beaches in our sample over a five year period, it appeared that the range for the number of pollutant was between 5 and 10, with the vast majority of beaches only having 5 pollutants. Therefore, the proposed linear mixed model and other traditional modeling techniques are not appropriate for this research question using the data at hand. In the Discussion section, we suggest ways to collect data which could better address our goal of measuring pollution across US beaches using our proposed methods.

## How States Communicate with Citizens about Their Beaches

Below, one can observe how beaches in each state communicate with residents along with the environmental literacy ratings for each state. To change the method of communication, use the drop-down input to select the desired output.

```{r, echo=FALSE}
state_long_lat <- read_csv("state_long_lat.csv")
environmental_literacy <- read_csv("environmental_literacy.csv") %>%
  mutate(level = case_when(
    rating >= 0.5 ~ "Good",
    rating < 0.5 ~ "Poor"
  ),
  state = ifelse(state == "DC", "District of Columbia", state))

environmental_literacy <- left_join(environmental_literacy, state_long_lat, by = c("state" = "name")) %>%
  filter(!state %in% c("Hawaii", "Alaska"))

states_comms <- local_action_decision_procedures %>%
  filter(!state_code %in% c("AS", "GU", "MP", "VI")) %>%
  mutate(state_code = case_when(
    state_code %in% c("MK", "ST")~ "WA",
    state_code == "BR" ~ "WI",
    state_code == "GP" ~ "MN",
    TRUE ~ as.character(state_code))) %>%
  group_by(state_code) %>%
  summarize(prob_internet = mean(posted_on_internet), prob_newspaper = mean(published_in_newspaper),
            prob_phone = mean(sent_to_phone_l_ist), prob_radio = mean(announcedon_radio),
            prob_beach = mean(posted_atthe_beach), prob_tv = mean(announcedon_tv), 
            prob_request = mean(provided_resultson_request), prob_other = mean(other))

us = map_data("state")

#'x' is the column of a data.frame that holds 2 digit state codes
stateFromLower <-function(x) {
  #read 52 state codes into local variable [includes DC (Washington D.C. and PR (Puerto Rico)]
  st.codes<-data.frame(
    state=as.factor(c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA",
                      "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
                      "MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM",
                      "NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN",
                      "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")),
    full=as.factor(c("alaska","alabama","arkansas","arizona","california","colorado",
                     "connecticut","district of columbia","delaware","florida","georgia",
                     "hawaii","iowa","idaho","illinois","indiana","kansas","kentucky",
                     "louisiana","massachusetts","maryland","maine","michigan","minnesota",
                     "missouri","mississippi","montana","north carolina","north dakota",
                     "nebraska","new hampshire","new jersey","new mexico","nevada",
                     "new york","ohio","oklahoma","oregon","pennsylvania","puerto rico",
                     "rhode island","south carolina","south dakota","tennessee","texas",
                     "utah","virginia","vermont","washington","wisconsin",
                     "west virginia","wyoming"))
  )
  #create an nx1 data.frame of state codes from source column
  st.x<-data.frame(state=x)
  #match source codes with codes from 'st.codes' local variable and use to return the full state name
  refac.x<-st.codes$full[match(st.x$state,st.codes$state)]
  #return the full state names in the same order in which they appeared in the original source
  return(refac.x)
} 

# Define UI
ui <- fluidPage(
  tags$head(tags$style(HTML("
                                #final_text {
                                  text-align: center;
                                }
                                div.box-header {
                                  text-align: center;
                                }
                                "))),
  mainPanel(
    selectInput("method",
                "Method of Communication:",
                c("Posted on Internet" = "prob_internet",
                  "Posted at Beach" = "prob_beach",
                  "Announced in Newspaper" = "prob_newspaper",
                  "Announced on Radio" = "prob_radio",
                  "Announced on TV" = "prob_tv",
                  "Sent to Phone List" = "prob_phone",
                  "Results on Request" = "prob_request",
                  "Other" = "prob_other")),
    girafeOutput("distPlot", width="800px", height="600px"),
  )
        
   
)

states_comms$region <- stateFromLower(states_comms$state_code)
us_states_elec <- left_join(us, states_comms)


server <- function(input, output) {

    output$distPlot <- renderGirafe({
      p <- (ggplot(data=us_states_elec, aes_string(x="long", y="lat", group="group", fill=input$method)) +
        geom_polygon(color = "black", size = 0.1)+
        geom_point(data=environmental_literacy, aes(x=longitude, y=latitude, size = rating), color="black", inherit.aes = FALSE) +
        guides(size=guide_legend("Environmental Literacy Rating")) +
        scale_fill_gradientn(colours = c("lightpink", "lightgreen"), na.value = "lightgrey") +
        theme(strip.background = element_blank()) +
        theme_map(base_size = 12) + 
        labs(fill = "Percent of Beaches Using this Method of Communication") +
        theme(legend.position="bottom", legend.box="vertical", legend.margin=margin(), legend.justification = "center"))
      
      girafe(code = print(p))

    })
}

# Run the application 
shinyApp(ui = ui, server = server, options = list(height = 800))
```

# Discussion

# Appendix

# References

- [1] [National Ocean Service](https://aambpublicoceanservice.blob.core.windows.net/oceanserviceprod/about/nos_brochure.pdf)
- [2] [Lotze et al. "Public Perceptions of Marine Threats and Protection from around the World"](http://lotzelab.biology.dal.ca/wp-content/uploads/Lotze-etal_2018_OCMA_public-perceptions.pdf)
- [3] BEACON Data Source and Information: [BEACON 2.0](https://watersgeo.epa.gov/beacon2/)
- [4] University of Tennessee - Knoxville Data Source: [Ruggiero, Karena. "A Criteria-Based Evaluation of Environmental Literacy Plans in the
United States"](https://trace.tennessee.edu/cgi/viewcontent.cgi?article=5129&context=utk_graddiss)